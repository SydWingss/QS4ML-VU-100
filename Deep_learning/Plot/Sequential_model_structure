digraph {
	graph [size="12,12"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1673172269424 [label="
 (1, 4)" fillcolor=darkolivegreen1]
	1673164609168 [label=SoftmaxBackward0]
	1673164616560 -> 1673164609168
	1673164616560 [label=AddmmBackward0]
	1673164609552 -> 1673164616560
	1673172267824 [label="layer.9.bias
 (4)" fillcolor=lightblue]
	1673172267824 -> 1673164609552
	1673164609552 [label=AccumulateGrad]
	1673159225264 -> 1673164616560
	1673159225264 [label=MulBackward0]
	1673159221472 -> 1673159225264
	1673159221472 [label=ReluBackward0]
	1673159220320 -> 1673159221472
	1673159220320 [label=AddmmBackward0]
	1673159223248 -> 1673159220320
	1673172267744 [label="layer.6.bias
 (32)" fillcolor=lightblue]
	1673172267744 -> 1673159223248
	1673159223248 [label=AccumulateGrad]
	1673159229200 -> 1673159220320
	1673159229200 [label=MulBackward0]
	1673159221184 -> 1673159229200
	1673159221184 [label=ReluBackward0]
	1673159221328 -> 1673159221184
	1673159221328 [label=AddmmBackward0]
	1673159226656 -> 1673159221328
	1673172267344 [label="layer.3.bias
 (64)" fillcolor=lightblue]
	1673172267344 -> 1673159226656
	1673159226656 [label=AccumulateGrad]
	1673159229056 -> 1673159221328
	1673159229056 [label=MulBackward0]
	1673159228240 -> 1673159229056
	1673159228240 [label=ReluBackward0]
	1673159224880 -> 1673159228240
	1673159224880 [label=AddmmBackward0]
	1673159224160 -> 1673159224880
	1673172267184 [label="layer.0.bias
 (128)" fillcolor=lightblue]
	1673172267184 -> 1673159224160
	1673159224160 [label=AccumulateGrad]
	1673159224544 -> 1673159224880
	1673159224544 [label=TBackward0]
	1673159217728 -> 1673159224544
	1673172267024 [label="layer.0.weight
 (128, 232)" fillcolor=lightblue]
	1673172267024 -> 1673159217728
	1673159217728 [label=AccumulateGrad]
	1673159227328 -> 1673159221328
	1673159227328 [label=TBackward0]
	1673159229008 -> 1673159227328
	1673172267264 [label="layer.3.weight
 (64, 128)" fillcolor=lightblue]
	1673172267264 -> 1673159229008
	1673159229008 [label=AccumulateGrad]
	1673159221616 -> 1673159220320
	1673159221616 [label=TBackward0]
	1673159228336 -> 1673159221616
	1673172267664 [label="layer.6.weight
 (32, 64)" fillcolor=lightblue]
	1673172267664 -> 1673159228336
	1673159228336 [label=AccumulateGrad]
	1673159225312 -> 1673164616560
	1673159225312 [label=TBackward0]
	1673159222576 -> 1673159225312
	1673172267904 [label="layer.9.weight
 (4, 32)" fillcolor=lightblue]
	1673172267904 -> 1673159222576
	1673159222576 [label=AccumulateGrad]
	1673164609168 -> 1673172269424
}
