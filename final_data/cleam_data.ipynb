{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from scipy.spatial.transform import Slerp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/taoyida/QS4ML-VU-100/final_data\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_merge_data_dir = os.getcwd() + '/raw_merge_data/'\n",
    "raw_merge_data_files = ['walk_merge_raw.csv', 'bike_merge_raw.csv', 'run_merge_raw.csv', 'sit_merge_raw.csv', 'syn_merge_raw.csv']\n",
    "\n",
    "#所有dataframe都存在这里\n",
    "df_list = []\n",
    "\n",
    "for i in range(len(raw_merge_data_files)):\n",
    "    df = pd.read_csv(raw_merge_data_dir + raw_merge_data_files[i], low_memory=False)\n",
    "    df_list.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "清洗手环数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "清洗手机系统数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "清洗MATLAB数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "# Define thresholds for specific columns\n",
    "thresholds = {\n",
    "    \"latitude\": (-90, 90),\n",
    "    \"longitude\": (-180, 180),\n",
    "    \"altitude\": (-500, 12000),\n",
    "    \"course\": (0, 360),\n",
    "    \"hacc\": (0, 100),  # Assuming max horizontal accuracy of 100 meters\n",
    "    \"speed\": (0, 300),  # Assuming max speed of 300 m/s\n",
    "}\n",
    "columns_to_check = [\n",
    "    \"altitude\",\n",
    "    \"course\",\n",
    "    \"hacc\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"speed\",\n",
    "]\n",
    "geolocation_columns = [\"altitude\", \"course\", \"latitude\", \"longitude\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_and_apply_thresholds(df, columns_to_check, thresholds):\n",
    "    for column in columns_to_check:\n",
    "        if column in df.columns:\n",
    "            threshold = thresholds.get(column, None)\n",
    "            if threshold is not None:\n",
    "                # Filter outliers, retain NaN values\n",
    "                df = df[(df[column].isna()) | ((df[column] >= threshold[0]) & (df[column] <= threshold[1]))]\n",
    "    return df\n",
    "\n",
    "def extract_XYZcolumns(df):\n",
    "    return df[[\"dateTime\", \"X\", \"Y\", \"Z\", \"Type\"]].copy()\n",
    "\n",
    "\n",
    "def extract_geolocation_columns(df):\n",
    "    return pd.concat([df[\"dateTime\"], df[geolocation_columns]], axis=1).copy()\n",
    "\n",
    "\n",
    "def extract_other_columns(df):\n",
    "    return df[[\"dateTime\", \"hacc\", \"speed\"]].copy()\n",
    "\n",
    "\n",
    "def extract_remaining_columns(df):\n",
    "    return df[\n",
    "        [\"dateTime\", \"BandAccX\", \"BandAccY\", \"BandAccZ\", \"rate\", \"rateZone\"]\n",
    "    ].copy()\n",
    "\n",
    "\n",
    "def XYZsplit(df):\n",
    "    # Drop rows where 'Type' is NaN\n",
    "    df = df.dropna(subset=[\"Type\"]).copy()\n",
    "\n",
    "    # Create columns for each type\n",
    "    types = df[\"Type\"].unique()\n",
    "    for t in types:\n",
    "        for col in [\"X\", \"Y\", \"Z\"]:\n",
    "            df.loc[:, f\"{t}_{col}\"] = df.apply(\n",
    "                lambda row: row[col] if row[\"Type\"] == t else None, axis=1\n",
    "            )\n",
    "\n",
    "    # Explicitly list columns to retain, no need for Position columns\n",
    "    columns_to_keep = [\"dateTime\"] + [\n",
    "        f\"{t}_{col}\"\n",
    "        for t in types\n",
    "        for col in [\"X\", \"Y\", \"Z\"]\n",
    "        if f\"{t}_{col}\" in df.columns\n",
    "    ]\n",
    "    df = df[columns_to_keep]\n",
    "    # Round the numeric columns to the desired decimal places\n",
    "    # Uncomment and modify the line below to set the desired decimal places\n",
    "    df = df.round(6)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Convert dateTime to pd.datetime\n",
    "def convert_to_datetime(df):\n",
    "    df[\"dateTime\"] = pd.to_datetime(df[\"dateTime\"])\n",
    "    # Create a new column for the second\n",
    "    df[\"Second\"] = df[\"dateTime\"].dt.floor(\"S\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def linear_columns_interpolation(df, columns):\n",
    "    for col in columns:\n",
    "        if df[col].notnull().sum() > 1:\n",
    "            df[col] = df[col].interpolate(\n",
    "                method=\"linear\", limit_direction=\"both\", limit_area=\"inside\"\n",
    "            )\n",
    "    return df\n",
    "\n",
    "\n",
    "def b_and_ffill_columns_interpolation(df, columns):\n",
    "    for col in columns:\n",
    "        df[col] = df[col].bfill().ffill()\n",
    "    return df\n",
    "\n",
    "\n",
    "def limited_columns_interpolation(df, columns):\n",
    "    for col in columns:\n",
    "        if df[col].notnull().sum() > 1:\n",
    "            df[col] = df[col].interpolate(method=\"linear\", limit_direction=\"both\", limit=20)\n",
    "            df[col] = df[col].fillna(0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def orientation_interpolation(df, columns):\n",
    "    # Check if the dataframe has the required columns\n",
    "    length = len(df)\n",
    "    if not all(col in df.columns for col in columns):\n",
    "        raise ValueError(\"DataFrame does not contain all required columns\")\n",
    "\n",
    "    # Convert the orientation columns to Rotation objects, handling NaNs\n",
    "    indices = []\n",
    "    rotations = []\n",
    "    for index, row in df[columns].iterrows():\n",
    "        if not row.isnull().any():\n",
    "            indices.append(index)\n",
    "            rotations.append(R.from_euler(\"xyz\", row, degrees=True))\n",
    "\n",
    "    # Check if there are at least two valid data points to perform interpolation\n",
    "    if len(indices) < 2:\n",
    "        raise ValueError(\"Not enough valid data points to perform interpolation\")\n",
    "\n",
    "    slerp = Slerp(indices, R.from_quat([r.as_quat() for r in rotations]))\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if df.iloc[i][columns].isnull().any():\n",
    "            # Ensure the interpolation index is within the valid range\n",
    "            if i >= indices[0] and i <= indices[-1]:\n",
    "                df.loc[i, columns] = slerp([i])[0].as_euler(\"xyz\", degrees=True)\n",
    "\n",
    "    # Fill any remaining NaNs (if interpolation limit is reached) with zeros or other strategy\n",
    "    df[columns] = df[columns].fillna(0)\n",
    "    df = df[:length]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XYZ变量命名规则：变量名_X\n",
    "mat数据由：XYZ数据，地理数据和剩余数据以及时间戳构成。上述所有的data均有时间戳做主元。清洗方法给在下面\n",
    "全部线性插值列：MagneticField_X\tMagneticField_Y\tMagneticField_Z\n",
    "有限线性插值列： Acceleration_X\tAcceleration_Y\tAcceleration_Z AngularVelocity_X\tAngularVelocity_Y\tAngularVelocity_Z\n",
    "方位角特殊插值： Orientation_X\tOrientation_Y\tOrientation_Z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mat_data(df):\n",
    "    print(\"Processing\" + str(df) + \"...\")\n",
    "    df = remove_outliers_and_apply_thresholds(df, columns_to_check, thresholds)\n",
    "    data = convert_to_datetime(df)\n",
    "    XYZdata = extract_XYZcolumns(data)\n",
    "    XYZdata_splited = XYZsplit(XYZdata)\n",
    "    remain_data = data.drop(columns=[\"X\", \"Y\", \"Z\", \"Type\"])\n",
    "    XYZdata_splited.set_index(\"dateTime\", inplace=True)\n",
    "    XYZdata_splited = XYZdata_splited.groupby(\"dateTime\").mean()\n",
    "    XYZdata_splited.drop(\n",
    "        columns=[\"Position_X\", \"Position_Y\", \"Position_Z\"], inplace=True\n",
    "    )\n",
    "    XYZdata_splited.reset_index(inplace=True)\n",
    "    t = pd.merge(remain_data, XYZdata_splited, on=\"dateTime\", how=\"outer\")\n",
    "    t = linear_columns_interpolation(\n",
    "        t,\n",
    "        columns=[\n",
    "            \"MagneticField_X\",\n",
    "            \"MagneticField_Y\",\n",
    "            \"MagneticField_Z\",\n",
    "            \"latitude\",\n",
    "            \"longitude\",\n",
    "            \"altitude\",\n",
    "            \"course\",\n",
    "            \"speed\",\n",
    "        ],\n",
    "    )\n",
    "    t = limited_columns_interpolation(\n",
    "        t,\n",
    "        columns=[\n",
    "            \"Acceleration_X\",\n",
    "            \"Acceleration_Y\",\n",
    "            \"Acceleration_Z\",\n",
    "            \"AngularVelocity_X\",\n",
    "            \"AngularVelocity_Y\",\n",
    "            \"AngularVelocity_Z\",\n",
    "        ],\n",
    "    )\n",
    "    t = orientation_interpolation(\n",
    "        t, columns=[\"Orientation_X\", \"Orientation_Y\", \"Orientation_Z\"]\n",
    "    )\n",
    "    t = b_and_ffill_columns_interpolation(t, columns=[\"hacc\"])\n",
    "    mat_columns = [\n",
    "        \"MagneticField_X\",\n",
    "        \"MagneticField_Y\",\n",
    "        \"MagneticField_Z\",\n",
    "        \"Acceleration_X\",\n",
    "        \"Acceleration_Y\",\n",
    "        \"Acceleration_Z\",\n",
    "        \"AngularVelocity_X\",\n",
    "        \"AngularVelocity_Y\",\n",
    "        \"AngularVelocity_Z\",\n",
    "        \"Orientation_X\",\n",
    "        \"Orientation_Y\",\n",
    "        \"Orientation_Z\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"altitude\",\n",
    "        \"course\",\n",
    "        \"hacc\",\n",
    "        \"speed\",\n",
    "    ]\n",
    "    t[mat_columns] = t[mat_columns].bfill().ffill()\n",
    "    t.drop(columns=[\"Second\"], inplace=True)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dfs = [process_mat_data(df) for df in df_list]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
